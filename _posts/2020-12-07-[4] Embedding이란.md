---
title:  "[NLP] Embedding 이란?"
categories:
  - NLP
tags:
  - STUDY
last_modified_at: 2020-12-07
---

# [4] Embedding이란?

## Embedding이란?

- 앞에서 살펴보았던 Vectorization의 치명적인 단점은 바로 단어나 문장들 사이의 관계에 대해서 설명하지 못한다는 것이다. 다음과 같은 예시가 있다.

![1.png](/assets/images/2020-12-07-[4]NLP/1.png)

- One Hot Encoding을 통해 각 5개의 토큰들이 고유의 벡터를 갖게 되었다. 하지만 사람이 보기엔 5개의 토큰들이 너무나도 명확하게 구분 된다. 지금까지 살펴 본 벡터화 방법들은 단어의 중요도나 문서안에서의 중요도는 구분 할 수 있지만 단어 사이의 유사도는 구별할 수 없었다.
- 이 때 사용되는 것이 Embedding 기법이며 Word2Vec을 비롯한 다양한 임베딩 기법들이 존재한다. 대략적인 아이디어는 다음과 같다.

![2.png](/assets/images/2020-12-07-[4]NLP/2.png)

- 위 예시처럼 비슷한 의미를 내포하고 있는 토큰들은 서로 가깝게, 그렇지 않는 토큰들은 서로 멀리 뿌리도록 하는 것이 임베딩의 목적이다. 검색 시스템, 감성 분석 등에서는 훌륭한 임베딩을 수행하는 것이 전체 문제 해결에 많은 영향을 준다.
- 임베딩 또한 하나의 모델을 의마하며 훈련이 필요하다. 데이터가 충분하고 시간이 많으면 소지한 데이터에 특화된 임베딩 모델을 학습시킬 수 있다. 보통은 PreTrained Emdding Model을 가져와서 사용한다.

---

## 1. Kreas Embedding Layer

- 기본적으로 가장 쉽고 빠르게 네트워크 모델에 임베딩 층을 주입할 수 있는 방식이다. 이 방법은 무작위로 특정 차원으로 입력 벡터들을 뿌린 후 학습을 통해 가중치들을 조정해 나가는 방식이다. 즉, 단어 사이의 관계를 반영하는 방법이 아니다.

```python
model = Sequential()
model.add(Embedding(vocab_size, 128, input_length = max_len))
```

## 2. word2vec

- word2vec의 핵심 아이디어는 "친구를 보면 그 사람을 알 수 있다."이다. 주변 단어와의 관계를 통해 해당 단어의 의미적 특성을 파악한다. [https://dreamgonfly.github.io/blog/word2vec-explained/](https://dreamgonfly.github.io/blog/word2vec-explained/)
- word2vec embedding matrix를 Keras의 Embedding에 주입하는 과정은 아래와 같다.

1. 구글의 사전 훈련된 word2vec bin 파일을 다운로드 한다.([https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit))

2. gensim 모듈과 bin파일을 활용해 word2vec 모델을 로드한다.

```python
import gensim
word2vec = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary = True)
```

3. Vocabulary에 있는 토큰의 벡터를 가져와 Embedding Matrix에 저장한다.

```python
embedding_matrix = np.zeros((vocab_size, 300)) #300차원의 임베딩 매트릭스 생성

for index, word in enumerate(vocabulary): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.
    if word in word2vec: #넘겨 받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 뜻)
        embedding_vector = word2vec[word] #해당 토큰에 해당하는 vector를 불러오고
        embedding_mxtrix[i] = embedding_vector #해당 위치의 embedding_mxtrix에 저장합니다.
    else:
        print("word2vec에 없는 단어입니다.")
        break
```

4. Keras Embedding Layer에 Embedding_Matrix를 가중치로 주어 이용한다.

```python
model = Sequential()
model.add(Embedding(vocab_size, 300,weights = [embedding_matrx], input_length = max_len))
```

---

## 3. glove

- glove는 word2vec의 단점을 보완하기 위해 등장하였다. word2vec이 사용자가 지정한 주변 단어의 개수에 대해서만 학습이 이루어지기 때문에 데이터 전체에 대한 정보를 담기 어렵다는 단점을 지적하였다. glove의 핵심 아이디어는 다음과 같다.

    각 토큰들 간의 유사성은 그대로 가져가면서 데이터 전체에 대한 빈도를 반영하자

- glove embedding matrix를 keras의 embedding matrix에 주입하는 방법은 다음과 같다.

    1. 사전 훈련된 벡터를 갖고 있는 txt 파일을 [이곳](https://drive.google.com/file/d/1yHGtccC2FV3_d6C6_Q4cozYSOgA7bG-e/view)에서 다운한다.

    2. txt 파일에 있는 단어와 벡터들을 glove dictionary에 저장한다.

    ```python
    2. # load the whole embedding into memory
    glove = dict()
    f = open('./glove.txt')
    for line in f:
        values = line.split()
        word = values[0]
        vector = asarray(values[1:], dtype='float32')
        glove[word] = vector
    f.close()
    ```

     3. vocabulary에 있는 토큰들의 벡터를 가져와 embedding matrix에 저장한다.

    ```python
    embedding_matrix = np.zeros((vocab_size, 300)) #300차원의 임베딩 매트릭스 생성

    for index, word in enumerate(vocabulary): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.
        if word in glove: #넘겨 받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 뜻)
            embedding_vector = glove[word] #해당 토큰에 해당하는 vector를 불러오고
            embedding_mxtrix[i] = embedding_vector #해당 위치의 embedding_mxtrix에 저장합니다.
        else:
            print("glove 없는 단어입니다.")
            break
    ```

     4. keras embedding layer에 embedding_matrix를 가중치로 주어 이용한다.

    ```python
    model = Sequential()
    model.add(Embedding(vocab_size, 300,weights = [embedding_matrx], input_length = max_len))
    ```

    ---

    ## 4. Fasttext

    - Fasttext의 핵심 아이디어는 단어 단위가 아닌 sub 단어를 단위로 사용한다.

        word2vec → "apple" 학습

        FastText → "ap", "pp", "pl, "le" 학습

    - 따라서 미리 학습되지 않는 단어들에 대한 Vector도 표현해준다는 장점이 있다.
        1. 사전 훈련된 bin 파일을 [이곳에서](https://drive.google.com/file/d/1yHGtccC2FV3_d6C6_Q4cozYSOgA7bG-e/view) 다운로드 한다.(glove에서 썼던 링크와 동일)
        2. 
        3. vec 파일을 gensim을 활용하여 읽어온다.

        ```python
        from gensim.models.keyedvectors import KeyedVectors
        FastText = KeyedVectors.load_word2vec_format('./fasttext.bin', binary = True)
        ```

         

        3. vocabulary에 있는 토큰들의 벡터를 가져와 embedding matrix에 저장한다.

        ```python
        embedding_matrix = np.zeros((vocab_size, 300)) #300차원의 임베딩 매트릭스 생성

        for index, word in enumerate(vocabulary): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.
            if word in word2vec: #넘겨 받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 뜻)
                embedding_vector = word2vec[word] #해당 토큰에 해당하는 vector를 불러오고
                embedding_mxtrix[i] = embedding_vector #해당 위치의 embedding_mxtrix에 저장합니다.
        ```

         4. Keras embedding layer에 embedding_matrix를 가중치로 주어 이용한다.

        ```python
        model = Sequential()
        model.add(Embedding(vocab_size, 300,weights = [embedding_matrx], input_length = max_len))
        ```

        ---

    ## 참고 사이트

    [https://dacon.io/competitions/official/235658/codeshare/1847?page=1&dtype=recent&ptype=pub](https://dacon.io/competitions/official/235658/codeshare/1847?page=1&dtype=recent&ptype=pub)